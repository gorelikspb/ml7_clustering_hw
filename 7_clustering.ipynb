{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый пункт, который предлагается выполнить в рамках домашнего задания, имеет объявленную \"цену\" в баллах. Максимально возможная сумма – 10 баллов, а с учётом бонусных пунктов – 12 баллов. Выполнять все пункты не обязательно, можно сделать только часть. В большинстве пунктов ожидается, что вы напишете работающий код на Python; иногда надо будет писать комментарии в свободной форме – например, сравнивать несколько подходов к решению одной задачи. Там, где оставлены пустые клетки под ваши ответы, вы можете по своему усмотрению добавлять ещё клетки.\n",
    "\n",
    "* * *\n",
    "\n",
    "Эта лабораторная работа посвящена кластеризации. Мы будем работать с рукописными изображениями цифр, научимся их кластеризовать двумя разными методами (иерархическая кластеризация и алгоритм $K$-means), оценивать качество разбиения и выбирать оптимальное число кластеров, а также визуализировать промежуточные результаты.\n",
    "\n",
    "# 1. Получение данных\n",
    "\n",
    "Данные, с которыми мы будем работать, доступны в библиотеке scikit-learn (модуль называется `sklearn`) в подмодуле `datasets` через функцию, которая называется `load_digits`. Всего имеется 1797 наблюдений, каждое из них представляет чёрно-белую картинку 8 $\\times$ 8 пикселей. Эти картинки – распознанные рукописные цифры от 0 до 9. Образцов написания каждой цифры дано приблизительно поровну, около 180.\n",
    "\n",
    "Для удобства использования данных каждая картинка \"развёрнута\" в строку, так что NumPy-массив, в котором хранятся данные, имеет размерность 2 и величину 1797 $\\times$ 64 (а не, например, размерность 3 и величину 1797 $\\times$ 8 $\\times$ 8). Интенсивность цвета в каждом пикселе кодируется целым числом от 0 до 16.\n",
    "\n",
    "Кроме наблюдений (картинок), известны соответствующие им значения целевой переменной: какую цифру на самом деле изображает каждая картинка. Мы могли бы сразу сформулировать задачу обучения с учителем и предсказывать цифры по картинкам, но для целей этой лабораторной работы мы будем действовать по-другому: сделаем вид, что нам не известны истинные метки классов (т. е. цифры) и даже количество классов, и попробуем сгруппировать данные таким образом, чтобы качество кластеризации оказалось наилучшим, а затем посмотрим, насколько точно полученные кластеры совпадают с группами изображений одинаковых цифр.\n",
    "\n",
    "**(0.5 балла)** Загрузите данные. Добейтесь, чтобы в переменной `X` оказался массив наблюдений, содержащий 1797 $\\times$ 64 числа, а в переменной `y` – массив истинных меток классов, содержащий 1797 чисел.\n",
    "\n",
    "*Указания:*\n",
    "- Как загрузить данные, объяснено в справке к функции `load_digits`.\n",
    "- Размер массива хранится в атрибуте `shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.load_digits(n_class=10, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 балл)** Визуализируйте первые десять картинок, расположив их на сетке 3 $\\times$ 4 (в последнем ряду останутся пустые места). Добейтесь, чтобы фон картинок был белым, а изображения цифр – тёмными.\n",
    "\n",
    "*Указания:*\n",
    "- Не забудьте импортировать NumPy и Matplotlib.\n",
    "- Картинки 8 $\\times$ 8 можно либо достать готовыми из объекта, загруженного функцией `load_digits`, либо сделать самостоятельно из строк массива `X`. Во втором случае пользуйтесь методом `reshape`.\n",
    "- Чтобы изображение не было цветным, можно вызвать функцию `plt.gray`, прежде чем начать рисовать.\n",
    "- Располагать картинки на сетке умеет функция `plt.subplot`. Ознакомьтесь со справкой к ней.\n",
    "- По умолчанию число 0 кодирует чёрный цвет, а число 16 – белый цвет. Подумайте, как обратить цвета одной операцией над NumPy-массивом.\n",
    "- Выводить картинку на экран умеет функция `plt.imshow`. Ознакомьтесь со справкой к ней.\n",
    "- Если считаете нужным, можете отключить сглаживание – параметр `interpolation` у функции `plt.imshow`.\n",
    "- Если считаете нужным, можете отключить деления на координатных осях. За это отвечают функции `plt.xticks` и `plt.yticks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, axes = plt.subplots (nrows = 3, ncols = 4)\n",
    "plt.gray()\n",
    "for ax, image in zip(axes.flatten(), X[:10]):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(16-image.reshape((8,8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Кластеризация и оценка качества\n",
    "\n",
    "Мы будем использовать два популярных алгоритма: иерархическую кластеризацию и метод $K$ средних ($K$-means clustering). Эти и другие алгоритмы кластеризации доступны в библиотеке scikit-learn в подмодуле `cluster`. Иерархическая кластеризация называется `AgglomerativeClustering`, а метод $K$ средних – `KMeans`.\n",
    "\n",
    "Интерфейс у большинства алгоритмов в scikit-learn простой и единообразный:\n",
    "- Чтобы инициализировать модель, нужно создать экземпляр соответствующего класса со всеми необходимыми параметрами. Например, у кластеризаций единственный обязательный параметр называется `n_clusters`, это количество кластеров, которое мы хотим получить на выходе.\n",
    "- Инициализированную модель можно обучить, вызвав метод `fit`.\n",
    "- С помощью обученной модели можно предсказывать, вызывая метод `predict`.\n",
    "\n",
    "Как видно, этот интерфейс хорош только для задач обучения с учителем, в которых чётко разделены фазы обучения модели и предсказания с её помощью. У кластеризаций зато есть метод `fit_predict`, который разбивает входную выборку на кластеры и сразу же возвращает результаты разбиения.\n",
    "\n",
    "**(0.5 балла)** Используя каждый из двух методов, иерархическую кластеризацию и $K$ средних, получите разбиение массива `X` на 10 кластеров.\n",
    "\n",
    "*Указания:*\n",
    "- Оба раза должен получиться массив из 1797 чисел – номеров кластеров.\n",
    "- `KMeans` делает несколько (по умолчанию 10) запусков со случайными центрами и из полученных разбиений выводит лучшее в терминах среднего внутрикластерного расстояния. Чтобы улучшить качество предсказаний, можно увеличить число запусков, например, до 100. Это параметр `n_init` в конструкторе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 9, 4, ..., 4, 1, 4], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clusters = AgglomerativeClustering(n_clusters = 10)\n",
    "clfp = clusters.fit_predict(X)\n",
    "clfp\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 9, 9, ..., 9, 8, 8])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans (n_clusters = 10)\n",
    "kmfp = kmeans.fit_predict(X)\n",
    "kmfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmfp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ДАЛЬШЕ НЕ УСПЕЛ ¯\\_(ツ)_/¯ можно не смотреть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 балл)** Визуализируйте центры кластеров, полученных каждым из двух способов. Это опять должны быть картинки на сетке 3 $\\times$ 4 с белым фоном и тёмными контурами. Прокомментируйте: какой из двух алгоритмов даёт центры кластеров, больше похожие на типичные начертания цифр?\n",
    "\n",
    "*Указания:*\n",
    "- Центр кластера – это среднее по всем наблюдениям, входящим в кластер, т. е. по какому-то набору строк из `X`.\n",
    "- Чтобы выбрать наблюдения, входящие в кластер номер `i`, используйте индексацию по булевозначной маске. Саму маску можно получить из массива предсказанных номеров кластеров и числа `i` оператором `==`.\n",
    "- Усреднять NumPy-массив вдоль какой-нибудь из осей умеет функция `np.mean`. Ознакомьтесь со справкой к ней. Нам нужно усреднение по строкам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ситуации, когда истинное число кластеров неизвестно, подбирают оптимальное число кластеров. При этом учитывают две величины: внутрикластерное расстояние (чем меньше, тем лучше) и межкластерное расстояние (чем больше, тем лучше). Так как две эти величины не достигают оптимума одновременно, обычно оптимизируют какой-нибудь функционал от них. Один популярный функционал называется \"силуэт\" (silhouette). Вот как он вычисляется.\n",
    "\n",
    "Пусть $X$ – множество наблюдений, $M \\subset X$ – один из кластеров, на которые оно разбито в результате кластеризации, $\\rho$ – метрика на $X$. Выберем какое-нибудь одно наблюдение $x \\in M$. Обозначим $a(x)$ среднее расстояние от $x$ до точек $x'$ из того же кластера:\n",
    "$$\n",
    "a(x) = \\frac{1}{|M| - 1} \\sum_{x' \\in M,\\, x' \\ne x} \\rho(x,\\, x')\n",
    "$$\n",
    "\n",
    "Обозначим $b(x)$ минимум средних расстояний от $x$ до точек $x''$ из какого-нибудь другого кластера $N$:\n",
    "$$\n",
    "b(x) = \\min_{N \\ne M} \\frac{1}{|N|} \\sum_{x'' \\in N} \\rho(x,\\, x'')\n",
    "$$\n",
    "\n",
    "Силуэт – это разность межкластерного и внутрикластерного расстояний, нормированная до отрезка $[-1,\\, 1]$ и усреднённая по всем наблюдениям:\n",
    "$$\n",
    "\\frac{1}{|X|} \\sum_{x \\in X} \\frac{b(x) - a(x)}{\\max(a(x),\\, b(x))}\n",
    "$$\n",
    "\n",
    "В scikit-learn силуэт считается функцией `silhouette_score` из подмодуля `metrics`. На вход нужно передать массив наблюдений и результат кластеризации.\n",
    "\n",
    "**(1.5 балла)** Для числа $K$ от 2 до 20 включительно получите разбиение массива `X` на $K$ кластеров каждым из двух методов. Посчитайте силуэт. Посчитанные значения силуэта сохраните в переменную и визуализируйте в виде графика в координатах: число $K$ – значение силуэта. При каком числе кластеров достигается максимум силуэта?\n",
    "\n",
    "*Указания:*\n",
    "- Не забудьте, что функция `range` не захватывает правый конец диапазона.\n",
    "- Под значения силуэта можно завести два списка: один для иерархической кластеризации, другой для $K$ средних.\n",
    "- Рисовать графики умеет функция `plt.plot`. Ознакомьтесь со справкой к ней.\n",
    "- На одной картинке можно разместить несколько графиков, это просто несколько последовательных вызовов `plt.plot`.\n",
    "- Чтобы добавить легенду (подписи к графикам), можно воспользоваться функцией `plt.legend`. Местоположение легенды контролируется параметром `loc`.\n",
    "- Чтобы подписать координатные оси, можно воспользоваться функциями `plt.xlabel` и `plt.ylabel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда известно \"правильное\" (в каком-нибудь смысле) разбиение на кластеры, результат кластеризации можно сравнить с ним, используя такие меры, как однородность (homogeneity), полнота (completeness) и их среднее гармоническое – $V$-мера. Определения этих величин довольно громоздкие и основаны на понятии [энтропии распределения вероятностей](https://ru.wikipedia.org/wiki/Информационная_энтропия); подробности излагаются в [этой статье](http://aclweb.org/anthology/D/D07/D07-1043.pdf). На практике достаточно знать, что однородность, полнота и $V$-мера заключены между нулём и единицей – чем больше, тем лучше.\n",
    "\n",
    "Так как мы знаем, какую цифру на самом деле изображает каждая картинка (это массив `y`), мы можем использовать однородность, полноту и $V$-меру для оценки качества кластеризации. Функции для вычисления этих величин доступны в scikit-learn, в подмодуле `metrics`, под названиями `homogeneity_score`, `completeness_score`, `v_measure_score`. Как вариант, можно использовать функцию `homogeneity_completeness_v_measure`, которая возвращает сразу тройку чисел.\n",
    "\n",
    "**(1 балл)** Повторите предыдущее задание, используя $V$-меру вместо силуэта. При каком числе кластеров достигается максимум $V$-меры?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Снижение размерности признакового пространства\n",
    "\n",
    "Иногда, особенно когда признаков много и не все они одинаково информативные, бывает полезно снизить размерность признакового пространства, то есть вместо $d$ исходных признаков перейти к рассмотрению $d' \\ll d$ новых признаков. Данные были представлены матрицей $n$ наблюдений $\\times$ $d$ исходных признаков, а теперь будут представлены матрицей $n$ наблюдений $\\times$ $d'$ новых признаков.\n",
    "\n",
    "Есть два популярных подхода к снижению размерности:\n",
    "- отобрать (select) новые признаки из числа имеющихся;\n",
    "- извлечь (extract) новые признаки, преобразуя старые, например, сделать $d'$ различных линейных комбинаций столбцов исходной матрицы $n \\times d$.\n",
    "\n",
    "Одним из широко используемых методов извлечения признаков является сингулярное разложение матрицы (singular value decomposition, SVD). Этот метод позволяет сконструировать любое число $d' \\le d$ новых признаков таким образом, что они будут, в определённом смысле, максимально информативными. Математические детали сейчас не важны; познакомиться с ними можно, например, [здесь](https://www.coursera.org/learn/mathematics-and-python/lecture/L9bCV/razlozhieniia-matrits-v-proizviedieniie-singhuliarnoie-razlozhieniie)\n",
    "(по-русски) или [здесь](https://www.youtube.com/watch?v=P5mlg91as1c) (по-английски).\n",
    "\n",
    "В scikit-learn есть несколько реализаций сингулярного разложения. Мы будем использовать класс `TruncatedSVD` из подмодуля `decomposition`. В конструктор этого класса достаточно передать один параметр `n_components` – желаемое число новых признаков. Метод `fit_transform` принимает матрицу и возвращает новую матрицу с таким же количеством строк, как прежде, и количеством столбцов, равным числу новых признаков.\n",
    "\n",
    "<u>Замечание:</u> Сингулярное разложение матрицы $M$ обычно пишут в виде $M = U \\Sigma V^{*}$, где $U$, $\\Sigma$ и $V$ – некие матрицы с хорошими свойствами. То, что возвращает алгоритм `TruncatedSVD`, – это сколько-то (сколько мы хотим получить) первых столбцов матрицы $U$.\n",
    "\n",
    "**(1.5 балла)** Выполните сингулярное разложение матрицы `X`, оставляя 2, 5, 10, 20 признаков. В каждом случае выполните иерархическую и $K$-means кластеризацию преобразованных данных (число кластеров примите равным 10). Посчитайте значения силуэта и $V$-меры. Удалось ли при каком-нибудь $d'$ получить силуэт и / или $V$-меру лучше, чем на исходных данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другая популярная техника снижения размерности, которая особенно хорошо подходит для работы с картинками, – это алгоритм t-distributed stochastic neighbor embeddings, сокращённо tSNE. В отличие от сингулярного разложения, это преобразование нелинейное. Его основная идея – отобразить точки из пространства размерности $d$ в пространство размерности 2 или 3 (обычно 2, то есть на плоскость) таким образом, чтобы как можно точнее сохранить расстояния. Математические детали есть, например, [здесь](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding), но они нетривиальны.\n",
    "\n",
    "В библиотеке scikit-learn реализацией tSNE является класс `TSNE` в подмодуле `manifold`. В конструктор можно передать параметр `n_components`, а можно и не передавать: по умолчанию он равен 2. Метод `fit_transform` работает аналогично тому, как и у `TruncatedSVD`.\n",
    "\n",
    "<u>Замечание:</u> В последние годы вместо tSNE на практике часто используется [UMAP](https://github.com/lmcinnes/umap), более быстрый алгоритм с похожими свойствами. В этой лабораторной работе не предлагается использовать UMAP, так как это потребовало бы установить ещё одну зависимость -- библиотеку `umap-learn`. Желающие могут проделать задания на tSNE с использованием UMAP; в этом случае обратите внимание на параметры `n_neighbors` и `min_dist`, которыми определяется вид проекции.\n",
    "\n",
    "**(0.5 балла)** Выполните tSNE-преобразование матрицы `X`, оставив 2 признака. Визуализируйте данные, преобразованные таким образом, в виде точечной диаграммы: первый признак вдоль горизонтальной оси, второй признак вдоль вертикальной оси. Подсветите разными цветами группы точек, соответствующих разным цифрам.\n",
    "\n",
    "*Указания:*\n",
    "- Точечную диаграмму умеет рисовать функция `plt.scatter`. Ознакомьтесь со справкой к ней.\n",
    "- За цвета точек отвечает параметр `c` у функции `plt.scatter`. Передать в него надо истинные метки классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 балл)** Для tSNE-преобразованных данных с 2 признаками выполните иерархическую и $K$-means кластеризацию (число кластеров примите равным 10). Посчитайте значения силуэта и $V$-меры. Удалось ли получить силуэт и / или $V$-меру лучше, чем на исходных данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 балла)** Для самого лучшего разбиения, которое вам удалось получить (на ваше усмотрение, лучшего в терминах силуэта или $V$-меры), опять визуализируйте картинками центры кластеров. Удалось ли добиться, чтобы каждый кластер соответствовал какой-нибудь одной цифре?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Итоги, бонус\n",
    "\n",
    "**(1 балл)** Напишите в свободной форме, какие выводы вы сделали из выполненной работы. Ответьте, как минимум, на следующие два вопроса:\n",
    "- Какой из двух методов даёт более осмысленные кластеры – иерархическая кластеризация или алгоритм $K$ средних? Зависит ли это от настроек каждого алгоритма? От критериев оценивания качества?\n",
    "- Удаётся ли улучшить качество кластеризации, снижая размерность признакового пространства?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Бонусные 2 балла)** Скачайте датасет [MNIST Handwritten Digits](http://yann.lecun.com/exdb/mnist). Как сделать это с помощью scikit-learn, написано [здесь](https://stackoverflow.com/a/60450028). MNIST Handwritten Digits – это 70 тысяч распознанных рукописных изображений цифр, каждое размером 28 $\\times$ 28 пикселей. Попробуйте прокластеризовать этот датасет и добиться как можно лучших значений силуэта и $V$-меры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
